{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this project, I*\n",
    "* Implemented forward computation for Multilayer Perceptron (MLP) neural network.\n",
    "* Implemented backpropagation algorithm to compute gradients for the network.\n",
    "* Implemented Gradient Descent optimization algorithm to train the network.\n",
    "* Trained and tested the network on a synthetic dataset for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required packages\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "def reset_seed():\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "\n",
    "def initialize_parameters(input_dim, output_dim):\n",
    "    W = np.random.randn(output_dim, input_dim) * 0.01\n",
    "    b = np.random.randn(output_dim) * 0.01\n",
    "    return W, b\n",
    "\n",
    "def initialize_parameters_xavier(input_dim, output_dim):\n",
    "    W = np.random.randn(output_dim, input_dim) / np.sqrt(input_dim)\n",
    "    b = np.zeros(output_dim)\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_linear(x: np.ndarray, W: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Performs the linear transformation.\n",
    "    x: input data (shape: D)\n",
    "    W: weight matrix (shape: H x D)\n",
    "    b: bias vector (shape: H)\n",
    "    Returns:\n",
    "    out: result of linear transformation (shape: H).\n",
    "    \"\"\"\n",
    "    out = np.matmul(W, x) + b\n",
    "    return out\n",
    "\n",
    "def forward_pass_relu(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Performs the ReLU activation.\n",
    "    x: input data (shape: H)\n",
    "    Returns:\n",
    "    out: result of ReLU activation (shape: H).\n",
    "    \"\"\"\n",
    "    v = (x > 0).astype(int)\n",
    "    out = v * x\n",
    "    return out\n",
    "\n",
    "def forward_pass_sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Performs the sigmoid activation.\n",
    "    x: input data (shape: H)\n",
    "    Returns:\n",
    "    out: result of sigmoid activation (shape: H).\n",
    "    \"\"\"\n",
    "    out = 1 / (1 + np.exp(-x))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x, W1, b1, W2, b2, W3, b3):\n",
    "    \"\"\"Complete forward pass through the 2-hidden layer MLP.\n",
    "    x: input data (shape: D)\n",
    "    W1, b1: weights and biases for first hidden layer (shape: H x D, H)\n",
    "    W2, b2: weights and biases for second hidden layer (shape: H x H, H)\n",
    "    W3, b3: weights and biases for output layer (shape: H x C, C)\n",
    "    Returns:\n",
    "    out: result of the forward pass (shape: C).\n",
    "    cache: a dictionary containing intermediate outputs of forward pass\n",
    "    \"\"\"\n",
    "    # First hidden layer\n",
    "    z1 = forward_pass_linear(x, W1, b1)\n",
    "    h1 = forward_pass_relu(z1)\n",
    "\n",
    "    # Second hidden layer\n",
    "    z2 = forward_pass_linear(h1, W2, b2)\n",
    "    h2 = forward_pass_relu(z2)\n",
    "\n",
    "    # Output layer\n",
    "    z3 = forward_pass_linear(h2, W3, b3)\n",
    "    out = forward_pass_sigmoid(z3)\n",
    "\n",
    "    cache = {\n",
    "        'z1': z1, 'h1': h1,\n",
    "        'z2': z2, 'h2': h2,\n",
    "        'z3': z3\n",
    "    }\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_binary_cross_entropy_loss(out: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Performs binary cross entropy loss computation.\n",
    "    out: model output between 0 and 1 (shape: C)\n",
    "    target: true label between 0 and 1 (shape: C)\n",
    "    Returns:\n",
    "    loss: binary cross entropy loss (shape: C).\n",
    "    \"\"\"\n",
    "\n",
    "    loss = -(target*np.log(out) + (1-target)*np.log(1 - out))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "D, H, C = 2, 3, 1\n",
    "reset_seed()\n",
    "W1, b1 = initialize_parameters(D, H)\n",
    "W2, b2 = initialize_parameters(H, H)\n",
    "W3, b3 = initialize_parameters(H, C)\n",
    "\n",
    "# Get a random input and target\n",
    "x = np.random.randn(D)\n",
    "target = np.random.randint(2, size=C)\n",
    "\n",
    "# Compute forward pass\n",
    "out, cache = forward_pass(x, W1, b1, W2, b2, W3, b3)\n",
    "loss = forward_pass_binary_cross_entropy_loss(out, target)\n",
    "\n",
    "# Compare with PyTorch reference output\n",
    "ref_out = np.array([0.49858720162952513])\n",
    "ref_loss = np.array([0.6903255683135151])\n",
    "assert np.allclose(out, ref_out), f'output {out} different from reference output {ref_out} within error tolerance, please check your implementation.'\n",
    "print(f'Test passed, output {out} equal to reference output {ref_out} within error tolerance.')\n",
    "assert np.allclose(loss, ref_loss), f'loss {loss} different from reference loss {ref_loss} within error tolerance, please check your implementation.'\n",
    "print(f'Test passed, loss {loss} equal to reference output {ref_loss} within error tolerance.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass_binary_cross_entropy_loss(out: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Backward pass for binary cross entropy loss.\n",
    "    out: model output between 0 and 1 (shape: C)\n",
    "    target: true label between 0 and 1 (shape: C)\n",
    "    Returns:\n",
    "    dout: gradient w.r.t. out (shape: C).\n",
    "    \"\"\"\n",
    "    dout = - target / out + (1 - target) / (1 - out)\n",
    "    return dout\n",
    "\n",
    "\n",
    "def backward_pass_sigmoid(dout, x):\n",
    "    \"\"\"Backward pass for the sigmoid activation.\n",
    "    dout: Upstream gradient (shape: H)\n",
    "    x: input data (shape: H)\n",
    "    Returns:\n",
    "    dx: gradient w.r.t x (shape: H)\n",
    "    \"\"\"\n",
    "    dx = dout * np.exp(-x) / np.square((1 + np.exp(-x)))\n",
    "    return dx\n",
    "\n",
    "\n",
    "def backward_pass_relu(dout, x):\n",
    "    \"\"\"Backward pass for the ReLU activation.\n",
    "    dout: Upstream gradient (shape: H)\n",
    "    x: input data (shape: H)\n",
    "    Returns:\n",
    "    dx: gradient w.r.t x (shape: H)\n",
    "    \"\"\"\n",
    "    dx = dout * (x > 0).astype(int)\n",
    "    return dx\n",
    "\n",
    "\n",
    "def backward_pass_linear(dout, x, W, b):\n",
    "    \"\"\"Backward pass for the linear layer.\n",
    "    dout: Upstream gradient (shape: H)\n",
    "    x: input data (shape: D)\n",
    "    W: weight matrix (shape: H x D)\n",
    "    b: bias vector (shape: H)\n",
    "    Returns:\n",
    "    dx: gradient w.r.t x (shape: D)\n",
    "    dW: gradient w.r.t W (shape: H x D)\n",
    "    db: gradient w.r.t b (shape: H)\n",
    "    \"\"\"\n",
    "    dx = np.dot(dout, W)\n",
    "    dW = np.outer(dout, x)\n",
    "    db = dout\n",
    "    return dx, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(dout, x, W1, b1, W2, b2, W3, b3, cache):\n",
    "    \"\"\"Complete backward pass through the 2-hidden layer MLP.\n",
    "    dout: Upstream gradient (usually gradient of loss w.r.t output, shape: 1)\n",
    "    x: input data (shape: D)\n",
    "    W1, b1, W2, b2, W3, b3: weights and biases\n",
    "    cache: a dictionary containing 'z1', 'h1', 'z2', 'h2', 'z3' which are intermediate outputs of forward pass\n",
    "    Returns:\n",
    "    gradients: a dictionary containing gradients for all weights and biases\n",
    "    \"\"\"\n",
    "    # Output layer gradients\n",
    "    dz3 = backward_pass_sigmoid(dout, cache['z3'])\n",
    "    dh2, dW3, db3 = backward_pass_linear(dz3, cache['h2'], W3, b3)\n",
    "\n",
    "    # Second hidden layer gradients\n",
    "    dz2 = backward_pass_relu(dh2, cache['z2'])\n",
    "    dh1, dW2, db2 = backward_pass_linear(dz2, cache['h1'], W2, b2)\n",
    "\n",
    "    # First hidden layer gradients\n",
    "    dz1 = backward_pass_relu(dh1, cache['z1'])\n",
    "    dx, dW1, db1 = backward_pass_linear(dz1, x, W1, b1)\n",
    "\n",
    "    gradients = {\n",
    "        'dW1': dW1, 'db1': db1,\n",
    "        'dW2': dW2, 'db2': db2,\n",
    "        'dW3': dW3, 'db3': db3\n",
    "    }\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "D, H, C = 2, 3, 1\n",
    "reset_seed()\n",
    "W1, b1 = initialize_parameters(D, H)\n",
    "W2, b2 = initialize_parameters(H, H)\n",
    "W3, b3 = initialize_parameters(H, C)\n",
    "\n",
    "# Get a random input and target\n",
    "x = np.random.randn(D)\n",
    "target = np.random.randint(2, size=C)\n",
    "\n",
    "# Compute forward pass\n",
    "out, cache = forward_pass(x, W1, b1, W2, b2, W3, b3)\n",
    "loss = forward_pass_binary_cross_entropy_loss(out, target)\n",
    "\n",
    "# Compute backward pass\n",
    "dout = backward_pass_binary_cross_entropy_loss(out, target)\n",
    "gradients = backward_pass(dout, x, W1, b1, W2, b2, W3, b3, cache)\n",
    "\n",
    "# Compare with reference gradients\n",
    "ref_dout = np.array([1.99436473])\n",
    "ref_dW1 = np.array([[4.4305505e-06, -4.5973820e-05], [0, 0], [0, 0]])\n",
    "ref_db1 = np.array([3.9942725e-05, 0, 0])\n",
    "ref_dW2 = np.array([[0, 0, 0], [0, 0, 0], [-0.0001274, 0, 0]])\n",
    "ref_db2 = np.array([[0, 0, -0.00710361]])\n",
    "ref_dW3 = np.array([[0, 0, 0.00725726]])\n",
    "ref_db3 = np.array([0.4985872])\n",
    "ref_gradients = {\n",
    "    'dW1': ref_dW1, 'db1': ref_db1,\n",
    "    'dW2': ref_dW2, 'db2': ref_db2,\n",
    "    'dW3': ref_dW3, 'db3': ref_db3\n",
    "}\n",
    "assert np.allclose(dout, ref_dout), f'gradient dout:\\n{dout}\\ndifferent from reference gradient\\n{ref_dout}\\nwithin error tolerance, please check your implementation.'\n",
    "print(f'Test passed, gradient dout equal to reference gradient within error tolerance.')\n",
    "for name, grad in gradients.items():\n",
    "    ref_grad = ref_gradients[name]\n",
    "    assert np.allclose(grad, ref_grad), f'gradient {name}:\\n{grad}\\ndifferent from reference gradient\\n{ref_grad}\\nwithin error tolerance, please check your implementation.'\n",
    "    print(f'Test passed, gradient {name} equal to reference gradient within error tolerance.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate):\n",
    "    \"\"\"A single step of gradient descent update.\n",
    "    W1, b1, W2, b2, W3, b3: weights and biases\n",
    "    dW1, db1, dW2, db2, dW3, db3: gradient of the object function w.r.t. weights and biases\n",
    "    learning_rate: learning rate\n",
    "    Returns:\n",
    "    W1, b1, W2, b2, W3, b3: updated weights and biases\n",
    "    \"\"\"\n",
    "    W1 = W1 - dW1 * learning_rate\n",
    "    W2 = W2 - dW2 * learning_rate\n",
    "    W3 = W3 - dW3 * learning_rate\n",
    "    b1 = b1 - db1 * learning_rate\n",
    "    b2 = b2 - db2 * learning_rate\n",
    "    b3 = b3 - db3 * learning_rate\n",
    "    return W1, b1, W2, b2, W3, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to train the network\n",
    "def train_network(X_train, Y_train, W1, b1, W2, b2, W3, b3, gradient_steps, learning_rate):\n",
    "\n",
    "    # Run training with gradient descent\n",
    "    for gradient_step in range(gradient_steps):\n",
    "        loss_list = []\n",
    "        gradients_list = []\n",
    "\n",
    "        for x, target in zip(X_train, Y_train):\n",
    "            # Compute forward pass\n",
    "            out, cache = forward_pass(x, W1, b1, W2, b2, W3, b3)\n",
    "            loss = forward_pass_binary_cross_entropy_loss(out, target)\n",
    "\n",
    "            # Compute backward pass\n",
    "            dout = backward_pass_binary_cross_entropy_loss(out, target)\n",
    "            gradients = backward_pass(dout, x, W1, b1, W2, b2, W3, b3, cache)\n",
    "\n",
    "            loss_list.append(loss)\n",
    "            gradients_list.append(gradients)\n",
    "\n",
    "        # Average gradients over dataset\n",
    "        dW1 = np.mean(np.stack([gradients['dW1'] for gradients in gradients_list], axis=0), axis=0)\n",
    "        db1 = np.mean(np.stack([gradients['db1'] for gradients in gradients_list], axis=0), axis=0)\n",
    "        dW2 = np.mean(np.stack([gradients['dW2'] for gradients in gradients_list], axis=0), axis=0)\n",
    "        db2 = np.mean(np.stack([gradients['db2'] for gradients in gradients_list], axis=0), axis=0)\n",
    "        dW3 = np.mean(np.stack([gradients['dW3'] for gradients in gradients_list], axis=0), axis=0)\n",
    "        db3 = np.mean(np.stack([gradients['db3'] for gradients in gradients_list], axis=0), axis=0)\n",
    "\n",
    "        # Gradient descent step\n",
    "        W1, b1, W2, b2, W3, b3 = update_parameters(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate)\n",
    "\n",
    "        # Print loss every 100 gradient descent steps\n",
    "        if gradient_step % 100 == 0:\n",
    "            print(f\"Step {gradient_step}, Loss: {np.mean(loss_list)}\")\n",
    "\n",
    "    # Final loss for correctness test\n",
    "    final_loss = np.mean(loss_list)\n",
    "\n",
    "    return W1, b1, W2, b2, W3, b3, final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup train and test datasets\n",
    "X, Y = sklearn.datasets.make_moons(n_samples=500, noise=0.1, random_state=42)\n",
    "Y = np.expand_dims(Y, 1)\n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, Y, test_size=0.2, random_state=24)\n",
    "\n",
    "# Visualize dataset\n",
    "plt.figure()\n",
    "plt.title('Train data with true labels')\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=Y)\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=['y=0', 'y=1'])\n",
    "plt.show()\n",
    "\n",
    "# Initialize parameters\n",
    "D, H, C = X_train.shape[-1], 32, Y_train.shape[-1]\n",
    "reset_seed()\n",
    "W1, b1 = initialize_parameters_xavier(D, H)\n",
    "W2, b2 = initialize_parameters_xavier(H, H)\n",
    "W3, b3 = initialize_parameters_xavier(H, C)\n",
    "\n",
    "# Run training with gradient descent\n",
    "gradient_steps = 1000\n",
    "learning_rate = 0.1\n",
    "W1, b1, W2, b2, W3, b3, final_loss = train_network(X_train, Y_train, W1, b1, W2, b2, W3, b3, gradient_steps, learning_rate)\n",
    "\n",
    "# Run predictions on the test data\n",
    "out_list = []\n",
    "for x, target in zip(X_test, Y_test):\n",
    "    out, _ = forward_pass(x, W1, b1, W2, b2, W3, b3)\n",
    "    out_list.append(out)\n",
    "predictions = (np.stack(out_list, axis=0) > 0.5).astype(int)\n",
    "accuracy = np.mean(predictions == Y_test)\n",
    "\n",
    "# Visualize predictions\n",
    "plt.figure()\n",
    "plt.title('Model predictions for test data')\n",
    "scatter = plt.scatter(X_test[:, 0], X_test[:, 1], c=predictions)\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=['pred=0', 'pred=1'])\n",
    "plt.show()\n",
    "\n",
    "# Test correctness\n",
    "ref_final_loss = 0.019107356285473685\n",
    "ref_accuracy = 1.\n",
    "assert np.allclose(final_loss, ref_final_loss), f'Final train loss: {final_loss} different from reference loss {ref_final_loss}, please check your implementation.'\n",
    "print(f'Test passed, final train loss: {final_loss}.')\n",
    "assert np.allclose(accuracy, ref_accuracy), f'Test accuracy: {100 * accuracy}% different from reference accuracy {100 * ref_accuracy}, please check your implementation.'\n",
    "print(f'Test passed, test accuracy: {100 * accuracy}%.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
