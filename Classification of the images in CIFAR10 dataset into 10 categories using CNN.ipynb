{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification using Convolutional Neural Networks (CNNs)\n",
    "- In this project, I classified the images in CIFAR10 dataset into 10 categories (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck) using Convolutional Neural Networks(CNNs).  \n",
    "\n",
    "- To this end, I implemented necessary network components (e.g. residual blocks) using nn. Module class and completed whole CNNs with those blocks. Then, I experimented those network architectures using train/testing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount drive https://datascience.stackexchange.com/questions/29480/uploading-images-folder-from-my-system-into-google-colab\n",
    "# login with your google account and type authorization code to mount on your google drive.\n",
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory path where `assignemnt1.ipynb` exists.\n",
    "# For example, if you saved `assignment1.ipynb` in `/gdrive/My Drive/CS471/assignment2` directory,\n",
    "# then set root = '/gdrive/My Drive/CS471/assignment2'\n",
    "root = '/gdrive/My Drive/CS71/assignment2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Block [(Illustration)](https://docs.google.com/drawings/d/17P4EZfF8ZoU6lllhg3quGHgDY8ol7wPEWOs3XutpHoU/edit?usp=sharing)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        \"\"\"\n",
    "        Initialize a basic multi-layer perceptron module components.\n",
    "        Illustration: https://docs.google.com/drawings/d/17P4EZfF8ZoU6lllhg3quGHgDY8ol7wPEWOs3XutpHoU/edit?usp=sharing\n",
    "\n",
    "        Instructions:\n",
    "            1. Implement an algorithm that initializes necessary components as illustrated in the above link.\n",
    "            2. Initialized network components will be referred in `forward` method\n",
    "               for constructing the dynamic computational graph.\n",
    "\n",
    "        Args:\n",
    "            1. in_channels (int): Number of channels in input.\n",
    "            2. out_channels (int): Number of channels to be produced.\n",
    "        \"\"\"\n",
    "        self.fc1 = nn.Linear(in_channels, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512,128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, out_channels)\n",
    "        self.bn3 = nn.BatchNorm1d(out_channels)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Feed-forward data 'x' through the module.\n",
    "\n",
    "        Instructions:\n",
    "            1. Construct the feed-forward computational graph as illustrated in the link\n",
    "               using the initialized components in __init__ method.\n",
    "\n",
    "        Args:\n",
    "            1. x (torch.FloatTensor): A tensor of shape (B, in_channels)\n",
    "            .\n",
    "        Returns:\n",
    "            1. output (torch.FloatTensor): An output tensor of shape (B, out_channels).\n",
    "        \"\"\"\n",
    "        output = self.act(self.bn1(self.fc1(x)))\n",
    "        output = self.act(self.bn2(self.fc2(output)))\n",
    "        output = self.act(self.bn3(self.fc3(output)))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Block[(Illustration)](https://docs.google.com/drawings/d/1ZrJAfY0GwfQ1IcmFuJaroFF5rj7FQZ2nZ3kjQXywhNs/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1,\n",
    "                 padding=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        \"\"\"\n",
    "        Initialize a basic convolutional layer module components.\n",
    "        Illustration: https://docs.google.com/drawings/d/1ZrJAfY0GwfQ1IcmFuJaroFF5rj7FQZ2nZ3kjQXywhNs/edit?usp=sharing\n",
    "\n",
    "        Args:\n",
    "            1. in_channels (int): Number of channels in the input.\n",
    "            2. out_channels (int): Number of channels produced.\n",
    "            3. kernel_size (int) : Size of the kernel used in conv layer (Default:3)\n",
    "            4. stride (int) : Stride of the convolution (Default:1)\n",
    "            5. padding (int) : Zero-padding added to both sides of the input (Default:1)\n",
    "        \"\"\"\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Feed-forward the data 'x' through the module.\n",
    "        Instructions:\n",
    "            1. Construct the feed-forward computational graph as illustrated in the link\n",
    "               using the initialized components in __init__ method.\n",
    "\n",
    "        Args:\n",
    "            1. x (torch.FloatTensor): A tensor of shape (B, in_channels, H, W).\n",
    "\n",
    "        Returns:\n",
    "            1. output (torch.FloatTensor): An output tensor of shape (B, out_channels, H, W).\n",
    "        \"\"\"\n",
    "        convo_lay = self.conv(x)\n",
    "        norm_lay = self.bn(convo_lay)\n",
    "        output = self.relu(norm_lay)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResBlockPlain [(Illustration)](https://docs.google.com/drawings/d/1bpWUIZ8uwGmfhu-tKAa01l1qBl5oouxCBZQTy5pkRaQ/edit?usp=sharing) (10pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlockPlain(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResBlockPlain, self).__init__()\n",
    "        \"\"\"Initialize a residual block module components.\n",
    "\n",
    "        Illustration: https://docs.google.com/drawings/d/1bpWUIZ8uwGmfhu-tKAa01l1qBl5oouxCBZQTy5pkRaQ/edit?usp=sharing\n",
    "\n",
    "        Instructions:\n",
    "            1. Implement an algorithm that initializes necessary components as illustrated in the above link.\n",
    "            2. Initialized network components will be referred in `forward` method\n",
    "               for constructing the dynamic computational graph.\n",
    "\n",
    "        Args:\n",
    "            1. in_channels (int): Number of channels in the input.\n",
    "        \"\"\"\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Feed-forward the data `x` through the network.\n",
    "\n",
    "        Instructions:\n",
    "            1. Construct the feed-forward computational graph as illustrated in the link\n",
    "               using the initialized components in __init__ method.\n",
    "\n",
    "        Args:\n",
    "            1. x (torch.FloatTensor): An tensor of shape (B, in_channels, H, W).\n",
    "\n",
    "        Returns:\n",
    "            1. output (torch.FloatTensor): An output tensor of shape (B, in_channels, H, W).\n",
    "        \"\"\"\n",
    "        temp = x\n",
    "        convo_lay1 = self.conv1(x)\n",
    "        norm_lay1 = self.bn1(convo_lay1)\n",
    "        output1 = self.relu(norm_lay1)\n",
    "        convo_lay2 = self.conv2(output1)\n",
    "        norm_lay2 = self.bn2(convo_lay2)\n",
    "        norm_lay2 += temp\n",
    "        output = self.relu(norm_lay2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResBlockBottleneck [(Illustration)](https://docs.google.com/drawings/d/1t55ibttP-X-8vPWYWFangN9pYdyDD6pVO7IJ_9tPgSA/edit?usp=sharing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlockBottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super(ResBlockBottleneck, self).__init__()\n",
    "        \"\"\"Initialize a residual block module components.\n",
    "\n",
    "        Illustration: https://docs.google.com/drawings/d/1t55ibttP-X-8vPWYWFangN9pYdyDD6pVO7IJ_9tPgSA/edit?usp=sharing\n",
    "\n",
    "        Instructions:\n",
    "            1. Implement an algorithm that initializes necessary components as illustrated in the above link.\n",
    "            2. Initialized network components will be referred in `forward` method\n",
    "               for constructing the dynamic computational graph.\n",
    "\n",
    "        Args:\n",
    "            1. in_channels (int): Number of channels in the input.\n",
    "            2. hidden_channels (int): Number of hidden channels produced by the first ConvLayer module.\n",
    "        \"\"\"\n",
    "        self.conv1 = nn.Conv2d(in_channels, hidden_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(hidden_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(hidden_channels)\n",
    "        self.conv3 = nn.Conv2d(hidden_channels, in_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Feed-forward the data `x` through the network.\n",
    "\n",
    "        Instructions:\n",
    "            1. Construct the feed-forward computational graph as illustrated in the link\n",
    "               using the initialized components in __init__ method.\n",
    "\n",
    "        Args:\n",
    "            1. x (torch.FloatTensor): An tensor of shape (B, in_channels, H, W).\n",
    "\n",
    "        Returns:\n",
    "            1. output (torch.FloatTensor): An output tensor of shape (B, in_channels, H, W).\n",
    "        \"\"\"\n",
    "        temp = x\n",
    "        convo_lay1 = self.conv1(x)\n",
    "        norm_lay1 = self.bn1(convo_lay1)\n",
    "        output1 = self.relu(norm_lay1)\n",
    "        convo_lay2 = self.conv2(output1)\n",
    "        norm_lay2 = self.bn2(convo_lay2)\n",
    "        output2 = self.relu(norm_lay2)\n",
    "        convo_lay3 = self.conv3(output2)\n",
    "        norm_lay3 = self.bn3(convo_lay3)\n",
    "        norm_lay3 += temp\n",
    "        output = self.relu(norm_lay3)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InceptionBlock[(Illustration)](https://docs.google.com/drawings/d/1ph_2qLcAWm_voJKWWAB-_Id0PtpJ73W4GEQQ33m3WpY/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        \"\"\"Initialize a basic InpcetionBlock module components.\n",
    "\n",
    "        Illustration: https://docs.google.com/drawings/d/1ph_2qLcAWm_voJKWWAB-_Id0PtpJ73W4GEQQ33m3WpY/edit?usp=sharing\n",
    "        Instructions:\n",
    "            1. Implement an algorithm that initializes necessary components as illustrated in the above link.\n",
    "            2. Initialized network components will be referred in `forward` method\n",
    "               for constructing the dynamic computational graph.\n",
    "\n",
    "        Args:\n",
    "            1. in_channels (int): Number of channels in the input.\n",
    "            2. out_channels (int): Number of channels in the final output.\n",
    "        \"\"\"\n",
    "        assert out_channels%8==0, 'out channel should be mutiplier of 8'\n",
    "\n",
    "        # Branch 1\n",
    "        self.branch1_conv1 = nn.Conv2d(in_channels, int(out_channels/4), kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.branch1_bn1 = nn.BatchNorm2d(int(out_channels/4))\n",
    "        self.branch1_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Branch 2\n",
    "        self.branch2_conv1 = nn.Conv2d(in_channels, int(out_channels/2), kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.branch2_bn1 = nn.BatchNorm2d(int(out_channels/2))\n",
    "        self.branch2_relu = nn.ReLU(inplace=True)\n",
    "        self.branch2_conv2 = nn.Conv2d(int(out_channels/2), int(out_channels/2), kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.branch2_bn2 = nn.BatchNorm2d(int(out_channels/2))\n",
    "\n",
    "        # Branch 3\n",
    "        self.branch3_conv1 = nn.Conv2d(in_channels, int(out_channels/8), kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.branch3_bn1 = nn.BatchNorm2d(int(out_channels/8))\n",
    "        self.branch3_relu = nn.ReLU(inplace=True)\n",
    "        self.branch3_conv2 = nn.Conv2d(int(out_channels/8), int(out_channels/8), kernel_size=5, stride=1, padding=2, bias=False)\n",
    "        self.branch3_bn2 = nn.BatchNorm2d(int(out_channels/8))\n",
    "\n",
    "        #Branch 4\n",
    "        self.branch4_maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.branch4_conv1 = nn.Conv2d(in_channels, int(out_channels/8), kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.branch4_bn1 = nn.BatchNorm2d(int(out_channels/8))\n",
    "        self.branch4_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Feed-forward the data `x` through the module.\n",
    "\n",
    "        Instructions:\n",
    "            1. Construct the feed-forward computational graph as illustrated in the link\n",
    "               using the initialized components in the __init__ method.\n",
    "\n",
    "        Args:\n",
    "            1. x (torch.FloatTensor): A tensor of shape (B, in_channels, H, W).\n",
    "\n",
    "        Returns:\n",
    "            1. output (torch.FloatTensor): An output tensor of shape (B, out_channels, H, W).\n",
    "\n",
    "        \"\"\"\n",
    "        #branch 1\n",
    "        bra1_convo_lay1 = self.branch1_conv1(x)\n",
    "        bra1_norm_lay1 = self.branch1_bn1(bra1_convo_lay1)\n",
    "        bra1_output1 = self.branch1_relu(bra1_norm_lay1)\n",
    "\n",
    "        #branch 2\n",
    "        bra2_convo_lay1 = self.branch2_conv1(x)\n",
    "        bra2_norm_lay1 = self.branch2_bn1(bra2_convo_lay1)\n",
    "        bra2_output1 = self.branch2_relu(bra2_norm_lay1)\n",
    "        bra2_convo_lay2 = self.branch2_conv2(bra2_output1)\n",
    "        bra2_norm_lay2 = self.branch2_bn2(bra2_convo_lay2)\n",
    "        bra2_output2 = self.branch2_relu(bra2_norm_lay2)\n",
    "\n",
    "        #branch 3\n",
    "        bra3_convo_lay1 = self.branch3_conv1(x)\n",
    "        bra3_norm_lay1 = self.branch3_bn1(bra3_convo_lay1)\n",
    "        bra3_output1 = self.branch3_relu(bra3_norm_lay1)\n",
    "        bra3_convo_lay2 = self.branch3_conv2(bra3_output1)\n",
    "        bra3_norm_lay2 = self.branch3_bn2(bra3_convo_lay2)\n",
    "        bra3_output2 = self.branch3_relu(bra3_norm_lay2)\n",
    "\n",
    "        #branch 4\n",
    "        bra4_max_lay1 = self.branch4_maxpool(x)\n",
    "        bra4_convo_lay1 = self.branch4_conv1(bra4_max_lay1)\n",
    "        bra4_norm_lay1 = self.branch4_bn1(bra4_convo_lay1)\n",
    "        bra4_output1 = self.branch4_relu(bra4_norm_lay1)\n",
    "\n",
    "        #Concatenation part\n",
    "        output = torch.cat((bra1_output1, bra2_output2, bra3_output2, bra4_output1), dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetworkExample(nn.Module):\n",
    "    def __init__(self, nf, block_type='mlp'):\n",
    "        super(MyNetworkExample, self).__init__()\n",
    "        \"\"\"Initialize an entire network module components.\n",
    "\n",
    "        Instructions:\n",
    "            1. Implement an algorithm that initializes necessary components.\n",
    "            2. Initialized network components will be referred in `forward` method\n",
    "               for constructing the dynamic computational graph.\n",
    "\n",
    "        Args:\n",
    "            1. nf (int): Number of input channels for the first nn.Linear Module. An abbreviation for num_filter.\n",
    "            2. block_type (str, optional): Type of blocks to use. ('mlp'. default: 'mlp')\n",
    "        \"\"\"\n",
    "        if block_type == 'mlp':\n",
    "            block = MLPBlock\n",
    "            # Since shape of input image is 3 x 32 x 32, the size of flattened input is 3*32*32.\n",
    "            self.mlp = block(3*32*32, nf)\n",
    "            self.fc = nn.Linear(nf, 10)\n",
    "        else:\n",
    "            raise Exception(f\"Wrong type of block: {block_type}.Expected : mlp\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Feed-forward the data `x` through the network.\n",
    "\n",
    "        Instructions:\n",
    "            1. Construct the feed-forward computational graph as illustrated in the link\n",
    "               using the initialized network components in __init__ method.\n",
    "        Args:\n",
    "            1. x (torch.FloatTensor): An image tensor of shape (B, 3, 32, 32).\n",
    "\n",
    "        Returns:\n",
    "            1. output (torch.FloatTensor): An output tensor of shape (B, 10).\n",
    "        \"\"\"\n",
    "        output = self.mlp(x.view(x.size()[0], -1))\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MyNetwork[(Illustration)](https://docs.google.com/drawings/d/1kk_oU9ZtaZlAchKruXykvDuBNpt-wVZ8pyBCIE7z3cU/edit?usp=sharing)\n",
    "\n",
    "There are two functions implemented in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self, nf, block_type='conv', num_blocks=[1, 1, 1]):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        \"\"\"Initialize an entire network module components.\n",
    "\n",
    "        Illustration: https://docs.google.com/drawings/d/1kk_oU9ZtaZlAchKruXykvDuBNpt-wVZ8pyBCIE7z3cU/edit?usp=sharing\n",
    "\n",
    "        Instructions:\n",
    "            1. Implement an algorithm that initializes necessary components as illustrated in the above link.\n",
    "            2. Initialized network components will be referred in `forward` method\n",
    "               for constructing the dynamic computational graph.\n",
    "\n",
    "        Args:\n",
    "            1. nf (int): Number of output channels for the first nn.Conv2d Module. An abbreviation for num_filter.\n",
    "            2. block_type (str, optional): Type of blocks to use. ('conv' | 'resPlain' | 'resBottleneck' | 'inception'. default: 'conv')\n",
    "            3. num_blocks (list or tuple, optional): A list or tuple of length 3.\n",
    "               Each item at i-th index indicates the number of blocks at i-th Layer.\n",
    "               (default: [1, 1, 1])\n",
    "        \"\"\"\n",
    "\n",
    "        self.block_type = block_type\n",
    "        # Define blocks according to block_type\n",
    "        if self.block_type == 'conv':\n",
    "            block = ConvBlock\n",
    "            block_args = lambda x: (x, x, 3, 1, 1)\n",
    "        elif self.block_type == 'resPlain':\n",
    "            block = ResBlockPlain\n",
    "            block_args = lambda x: (x,)\n",
    "        elif self.block_type == 'resBottleneck':\n",
    "            block = ResBlockBottleneck\n",
    "            block_args = lambda x: (x, x//2)\n",
    "        elif self.block_type == 'inception':\n",
    "            block = InceptionBlock\n",
    "            block_args = lambda x: (x, x)\n",
    "        else:\n",
    "            raise Exception(f\"Wrong type of block: {block_type}\")\n",
    "\n",
    "        # Define block layer by stacking multiple blocks.\n",
    "        # You don't need to modify it. Just use these block layers in forward function.\n",
    "        self.block1 = nn.Sequential(*[block(*block_args(nf)) for _ in range(num_blocks[0])])\n",
    "        self.block2 = nn.Sequential(*[block(*block_args(nf*2)) for _ in range(num_blocks[1])])\n",
    "        self.block3 = nn.Sequential(*[block(*block_args(nf*4)) for _ in range(num_blocks[2])])\n",
    "\n",
    "        # before block 1\n",
    "        self.conv1 = nn.Conv2d(3, nf, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(nf)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.max_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #before block 2\n",
    "        self.conv2 = nn.Conv2d(nf, nf*2, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(nf*2)\n",
    "        self.max_2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #before block 3\n",
    "        self.conv3 = nn.Conv2d(nf*2, nf*4, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(nf*4)\n",
    "        self.max_3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #after block3\n",
    "        self.avg_1 = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.flat = nn.Flatten()\n",
    "        self.line = nn.Linear(nf*4, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Feed-forward the data `x` through the network.\n",
    "\n",
    "        Instructions:\n",
    "            1. Construct the feed-forward computational graph as illustrated in the link\n",
    "               using the initialized network components in __init__ method.\n",
    "        Args:\n",
    "            1. x (torch.FloatTensor): An image tensor of shape (B, 3, 32, 32).\n",
    "\n",
    "        Returns:\n",
    "            1. output (torch.FloatTensor): An output tensor of shape (B, 10).\n",
    "        \"\"\"\n",
    "\n",
    "        convo_lay1 = self.conv1(x)\n",
    "        norm_lay1 = self.bn1(convo_lay1)\n",
    "        output1 = self.relu(norm_lay1)\n",
    "        max_pool_1 = self.max_1(output1)\n",
    "        new_block1 = self.block1(max_pool_1)\n",
    "        convo_lay2 = self.conv2(new_block1)\n",
    "        norm_lay2 = self.bn2(convo_lay2)\n",
    "        output2 = self.relu(norm_lay2)\n",
    "        max_pool_2 = self.max_2(output2)\n",
    "        new_block2 = self.block2(max_pool_2)\n",
    "        convo_lay3 = self.conv3(new_block2)\n",
    "        norm_lay3 = self.bn3(convo_lay3)\n",
    "        output3 = self.relu(norm_lay3)\n",
    "        max_pool_3 = self.max_3(output3)\n",
    "        new_block3 = self.block3(max_pool_3)\n",
    "        avg_pool_1 = self.avg_1(new_block3)\n",
    "        output = self.flat(avg_pool_1)\n",
    "        output = self.line(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations & Hyper-parameters\n",
    "\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "# set manual seeds\n",
    "torch.manual_seed(470)\n",
    "torch.cuda.manual_seed(470)\n",
    "\n",
    "args = edict()\n",
    "\n",
    "# basic options\n",
    "args.name = 'main'                   # experiment name.\n",
    "args.ckpt_dir = 'ckpts'              # checkpoint directory name.\n",
    "args.ckpt_iter = 1000                # how frequently checkpoints are saved.\n",
    "args.ckpt_reload = 'best'            # which checkpoint to re-load.\n",
    "args.gpu = True                      # whether or not to use gpu.\n",
    "\n",
    "# network options\n",
    "args.num_filters = 16                # number of output channels in the first nn.Conv2d module in MyNetwork.\n",
    "args.block_type = 'mlp'              # type of block. ('mlp' | 'conv' | 'resPlain' | 'resBottleneck' | 'inception').\n",
    "args.num_blocks = [5, 5, 5]          # number of blocks in each Layer.\n",
    "\n",
    "# data options\n",
    "args.dataroot = 'dataset/cifar10'    # where CIFAR10 images exist.\n",
    "args.batch_size = 128                # number of mini-batch size.\n",
    "\n",
    "# training options\n",
    "args.lr = 0.1                        # learning rate.\n",
    "args.epoch = 100                     # training epoch.\n",
    "\n",
    "# tensorboard options\n",
    "args.tensorboard = True              # whether or not to use tensorboard logging.\n",
    "args.log_dir = 'logs'                # to which tensorboard logs will be saved.\n",
    "args.log_iter = 100                  # how frequently logs are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic settings\n",
    "device = 'cuda' if torch.cuda.is_available() and args.gpu else 'cpu'\n",
    "\n",
    "result_dir = Path(root) / 'results'\n",
    "result_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "global_step = 0\n",
    "best_accuracy = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train/test data loaders\n",
    "# Use data augmentation in training set to mitigate overfitting.\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "    ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "    ])\n",
    "\n",
    "train_dataset = CIFAR10(args.dataroot, download=True, train=True, transform=train_transform)\n",
    "test_dataset = CIFAR10(args.dataroot, download=True, train=False, transform=test_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup tensorboard.\n",
    "if args.tensorboard:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir \"/gdrive/My Drive/{str(result_dir).replace('/gdrive/My Drive/', '')}\"\n",
    "else:\n",
    "    writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net, optimizer, scheduler, block_type, writer):\n",
    "    global_step = 0\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for epoch in range(args.epoch):\n",
    "        # Here starts the train loop.\n",
    "        net.train()\n",
    "        for batch_idx, (x, y) in enumerate(train_dataloader):\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            #  Send `x` and `y` to either cpu or gpu using `device` variable.\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            # Feed `x` into the network, get an output, and keep it in a variable called `logit`.\n",
    "            logit = net(x)\n",
    "\n",
    "            # Compute accuracy of this batch using `logit`, and keep it in a variable called 'accuracy'.\n",
    "            accuracy = (logit.argmax(1) == y).float().mean()\n",
    "\n",
    "            # Compute loss using `logit` and `y`, and keep it in a variable called `loss`.\n",
    "            loss = nn.CrossEntropyLoss()(logit, y)\n",
    "\n",
    "            # flush out the previously computed gradient.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # backward the computed loss.\n",
    "            loss.backward()\n",
    "\n",
    "            # update the network weights.\n",
    "            optimizer.step()\n",
    "\n",
    "            if global_step % args.log_iter == 0 and writer is not None:\n",
    "                # Log loss and accuracy values using `writer`. Use `global_step` as a timestamp for the log.\n",
    "                writer.add_scalar('train_loss', loss, global_step)\n",
    "                writer.add_scalar('train_accuracy', accuracy, global_step)\n",
    "\n",
    "            if global_step % args.ckpt_iter == 0:\n",
    "                # Save network weights in the directory specified by `ckpt_dir` directory.\n",
    "                torch.save(net.state_dict(), f'{ckpt_dir}/{global_step}.pt')\n",
    "\n",
    "        # Here starts the test loop.\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0.\n",
    "            test_accuracy = 0.\n",
    "            test_num_data = 0.\n",
    "            for batch_idx, (x, y) in enumerate(test_dataloader):\n",
    "                # Send `x` and `y` to either cpu or gpu using `device` variable..\n",
    "                x = x.to(device=device)\n",
    "                y = y.to(device=device)\n",
    "\n",
    "                # Feed `x` into the network, get an output, and keep it in a variable called `logit`.\n",
    "                logit = net(x)\n",
    "\n",
    "                # Compute loss using `logit` and `y`, and keep it in a variable called `loss`.\n",
    "                loss = nn.CrossEntropyLoss()(logit, y)\n",
    "\n",
    "                # Compute accuracy of this batch using `logit`, and keep it in a variable called 'accuracy'.\n",
    "                accuracy = (logit.argmax(dim=1) == y).float().mean()\n",
    "\n",
    "                test_loss += loss.item()*x.shape[0]\n",
    "                test_accuracy += accuracy.item()*x.shape[0]\n",
    "                test_num_data += x.shape[0]\n",
    "\n",
    "            test_loss /= test_num_data\n",
    "            test_accuracy /= test_num_data\n",
    "\n",
    "            if writer is not None:\n",
    "                # Log loss and accuracy values using `writer`. Use `global_step` as a timestamp for the log.\n",
    "                writer.add_scalar('test_loss', test_loss, global_step)\n",
    "                writer.add_scalar('test_accuracy', test_accuracy, global_step)\n",
    "\n",
    "                # Just for checking progress\n",
    "                print(f'Test result of epoch {epoch}/{args.epoch} || loss : {test_loss:.3f} acc : {test_accuracy:.3f} ')\n",
    "\n",
    "                writer.flush()\n",
    "\n",
    "            # Whenever `test_accuracy` is greater than `best_accuracy`, save network weights with the filename 'best.pt' in the directory specified by `ckpt_dir`.\n",
    "            if test_accuracy > best_accuracy:\n",
    "                best_accuracy = test_accuracy\n",
    "                torch.save(net.state_dict(), f'{ckpt_dir}/{block_type}_best.pt')\n",
    "\n",
    "        scheduler.step()\n",
    "    return best_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for weight initialization.\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.constant_(m.weight, 1)\n",
    "        torch.nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all block types we will use.\n",
    "block_types = ['mlp', 'conv','resPlain','resBottleneck','inception']\n",
    "\n",
    "# Create directory name.\n",
    "num_trial=0\n",
    "parent_dir = result_dir / f'trial_{num_trial}'\n",
    "while parent_dir.is_dir():\n",
    "    num_trial = int(parent_dir.name.replace('trial_',''))\n",
    "    parent_dir = result_dir / f'trial_{num_trial+1}'\n",
    "print(f'Logs and ckpts will be saved in : {parent_dir}')\n",
    "\n",
    "# Define networks\n",
    "networks = []\n",
    "for block_type in block_types:\n",
    "    if block_type == 'conv':\n",
    "        args.num_blocks = [10, 10, 10]\n",
    "    else:\n",
    "        args.num_blocks = [5, 5, 5]\n",
    "\n",
    "    if block_type == 'mlp':\n",
    "        network = MyNetworkExample(64, block_type).to(device)\n",
    "    else:\n",
    "        network = MyNetwork(args.num_filters, block_type, args.num_blocks).to(device)\n",
    "\n",
    "    network.apply(weight_init)\n",
    "    networks.append(network)\n",
    "\n",
    "# Count the number of parameters of the models.\n",
    "# You can use it as an indicator of whether you correctly implemented the model.\n",
    "\n",
    "correct_params = {'mlp' : 1649354, 'conv' : 510426, 'resPlain' : 510426, 'resBottleneck' : 113946, 'inception' : 124026}\n",
    "for block_type, net  in zip(block_types, networks):\n",
    "    # Print the number of parameters in each model.\n",
    "    num_parameters = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    print(f'# of parameters in {block_type} net : {num_parameters}')\n",
    "    print(f'Correct # of parameters in {block_type} net : {correct_params[block_type]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_accs = {}\n",
    "\n",
    "# Start training\n",
    "for block_type, net in zip(block_types, networks):\n",
    "    try:\n",
    "        args.name = block_type\n",
    "        # Define optimizer\n",
    "        optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=0.0001)\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50,80], gamma=0.5)\n",
    "\n",
    "        # Create directories for logs and ckechpoints.\n",
    "        ckpt_dir = parent_dir / args.name / args.ckpt_dir\n",
    "        ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "        log_dir = parent_dir / args.name / args.log_dir\n",
    "        log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Create tensorboard writer,\n",
    "        if args.tensorboard:\n",
    "            writer = SummaryWriter(log_dir)\n",
    "\n",
    "        # Call the train & test function.\n",
    "        t1 = time.time()\n",
    "        accuracy = train_net(net, optimizer, scheduler, block_type, writer)\n",
    "        t = time.time()-t1\n",
    "        print(f'Best test accuracy of {block_type} network : {accuracy:.3f} took {t:.3f} secs')\n",
    "        final_accs[f'{block_type}'] = accuracy*100\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# Print final best accuracies of the models.\n",
    "for key in final_accs.keys():\n",
    "    print(f'Best accuracy of {key} = {final_accs[key]:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_types = ['mlp', 'conv','resPlain','resBottleneck','inception']\n",
    "test_accs = {}\n",
    "test_params= {}\n",
    "\n",
    "for block_type, net in zip(block_types, networks):\n",
    "        ckpt_dir = parent_dir / block_type / args.ckpt_dir\n",
    "\n",
    "        # load weights from best checkpoints.\n",
    "        ckpt_path = f'{ckpt_dir}/{block_type}_best.pt'\n",
    "        try:\n",
    "            net.load_state_dict(torch.load(ckpt_path))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        # Measure test performance.\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            test_accuracy = 0.\n",
    "            test_num_data = 0.\n",
    "            for batch_idx, (x, y) in enumerate(test_dataloader):\n",
    "                # Send `x` and `y` to either cpu or gpu using `device` variable..\n",
    "                x = x.to(device=device)\n",
    "                y = y.to(device=device)\n",
    "\n",
    "                # Feed `x` into the network, get an output, and keep it in a variable called `logit`.\n",
    "                logit = net(x)\n",
    "\n",
    "                # Compute loss using `logit` and `y`, and keep it in a variable called `loss`.\n",
    "                loss = nn.CrossEntropyLoss()(logit, y)\n",
    "\n",
    "                # Compute accuracy of this batch using `logit`, and keep it in a variable called 'accuracy'.\n",
    "                accuracy = (logit.argmax(dim=1) == y).float().mean()\n",
    "\n",
    "                test_accuracy += accuracy.item()*x.shape[0]\n",
    "                test_num_data += x.shape[0]\n",
    "\n",
    "            # Average classification accuracy.\n",
    "            test_accuracy /= test_num_data\n",
    "\n",
    "            # Count the number of implemented models.\n",
    "            num_parameters = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "\n",
    "            test_accs[f'{block_type}'] = test_accuracy*100\n",
    "            test_params[f'{block_type}'] = num_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing final results.\n",
    "correct_accs = {'mlp' : 62.6,'conv' : 81.9,'resPlain' : 88.6, 'resBottleneck' : 86.5, 'inception' : 83.7}\n",
    "correct_params = {'mlp' : 1649354, 'conv' : 510426, 'resPlain' : 510426, 'resBottleneck' : 113946, 'inception' : 124026}\n",
    "\n",
    "print(' Method        | Accuracy   | # Params    | Expected Acc | Expected # Params  ')\n",
    "print('------------------------------------------------------------------------------')\n",
    "for block in block_types:\n",
    "        print(f' {block:14}| {str(test_accs[block])[:5]:11}| {str(test_params[block]):11} | {str(correct_accs[block])[:5]:13}| {str(correct_params[block]):12}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
